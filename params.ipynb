{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "params.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOg0gbAt5leVMvTnUxty6oT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbht98/attack_cat_or_dog_classification/blob/master/params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h9gZuzEGrXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "73cb0534-dc6a-4fff-e49a-a54bb3127642"
      },
      "source": [
        " !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 20 00:22:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Sp59mFFcIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "269011c7-7818-45fe-e6ff-942e95fdd23d"
      },
      "source": [
        "!git clone https://github.com/tensorflow/privacy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'privacy'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 1773 (delta 5), reused 9 (delta 3), pack-reused 1755\u001b[K\n",
            "Receiving objects: 100% (1773/1773), 593.80 KiB | 2.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1219/1219), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVtpAPnUFn7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "3f269efa-d82c-4569-a5d1-13de90a38b0d"
      },
      "source": [
        "%cd privacy\n",
        "!pip install -e ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/privacy\n",
            "Obtaining file:///content/privacy\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy==0.3.0) (1.1.0)\n",
            "Collecting dm-tree~=0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/48/10fb721334810081b7e6eebeba0d12e12126c76993e8c243062d2f56a89f/dm_tree-0.1.5-cp36-cp36m-manylinux1_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow-privacy==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from dm-tree~=0.1.1->tensorflow-privacy==0.3.0) (1.12.0)\n",
            "Installing collected packages: dm-tree, tensorflow-privacy\n",
            "  Found existing installation: tensorflow-privacy 0.2.2\n",
            "    Uninstalling tensorflow-privacy-0.2.2:\n",
            "      Successfully uninstalled tensorflow-privacy-0.2.2\n",
            "  Running setup.py develop for tensorflow-privacy\n",
            "Successfully installed dm-tree-0.1.5 tensorflow-privacy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAGdesBUFpqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b116570-8e10-4969-dfb7-f357d6973c0c"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBF1XNFFsFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "03d1f7e0-3e4c-4a82-ac17-f8b9e6fe2664"
      },
      "source": [
        "!pip install tensorflow_privacy\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_privacy in /content/privacy (0.3.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.4.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.1.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (0.1.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow_privacy) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from dm-tree~=0.1.1->tensorflow_privacy) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cApJOqsgFtzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4b439a1f-827f-4047-b744-d33e4001c3bb"
      },
      "source": [
        "def get_data():\n",
        "    \"\"\"Prepare MNIST data.\"\"\"\n",
        "    train, test = tf.keras.datasets.mnist.load_data()\n",
        "    train_data, train_labels = train\n",
        "    test_data, test_labels = test\n",
        "\n",
        "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "    train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "    test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "    train_labels = np.array(train_labels, dtype=np.int32)\n",
        "    test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "    assert train_data.min() == 0.\n",
        "    assert train_data.max() == 1.\n",
        "    assert test_data.min() == 0.\n",
        "    assert test_data.max() == 1.\n",
        "    return (train_data, train_labels), (test_data, test_labels)\n",
        "# Get MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = get_data()\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZVKHy3GFzWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "0988bb8c-2e9f-4a23-a736-7a598bcbdd53"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 200\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 200\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model = target_model_fn()\n",
        "target_model.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 2.2075 - acc: 0.3780 - val_loss: 2.0154 - val_acc: 0.5877\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.8780 - acc: 0.6869 - val_loss: 1.7718 - val_acc: 0.7613\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.7229 - acc: 0.7904 - val_loss: 1.6706 - val_acc: 0.8299\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6535 - acc: 0.8390 - val_loss: 1.6294 - val_acc: 0.8545\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6185 - acc: 0.8635 - val_loss: 1.5984 - val_acc: 0.8794\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5972 - acc: 0.8779 - val_loss: 1.5819 - val_acc: 0.8914\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5835 - acc: 0.8886 - val_loss: 1.5737 - val_acc: 0.8959\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5737 - acc: 0.8959 - val_loss: 1.5633 - val_acc: 0.9042\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5667 - acc: 0.9022 - val_loss: 1.5571 - val_acc: 0.9112\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5619 - acc: 0.9054 - val_loss: 1.5526 - val_acc: 0.9146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d656c37b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF8n78YEdaEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "5579f623-aaa1-4897-91d6-6cd5443a86a4"
      },
      "source": [
        "target_model.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=200)\n",
        "target_model.save('weights_mnist_batch_200_15.h5')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5580 - acc: 0.9085 - val_loss: 1.5480 - val_acc: 0.9188\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5540 - acc: 0.9117 - val_loss: 1.5451 - val_acc: 0.9207\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5505 - acc: 0.9146 - val_loss: 1.5428 - val_acc: 0.9206\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5479 - acc: 0.9168 - val_loss: 1.5399 - val_acc: 0.9250\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5457 - acc: 0.9193 - val_loss: 1.5375 - val_acc: 0.9267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Q9d9UHOxkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "60696463-7643-47b2-f70f-8d826e1997eb"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn600():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model600 = target_model_fn600()\n",
        "target_model600.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 2.0160 - acc: 0.5573 - val_loss: 1.7097 - val_acc: 0.8094\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6569 - acc: 0.8338 - val_loss: 1.6041 - val_acc: 0.8731\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5971 - acc: 0.8768 - val_loss: 1.5725 - val_acc: 0.8984\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5746 - acc: 0.8951 - val_loss: 1.5597 - val_acc: 0.9070\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5634 - acc: 0.9032 - val_loss: 1.5530 - val_acc: 0.9125\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5577 - acc: 0.9074 - val_loss: 1.5460 - val_acc: 0.9184\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5519 - acc: 0.9130 - val_loss: 1.5416 - val_acc: 0.9239\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5473 - acc: 0.9166 - val_loss: 1.5362 - val_acc: 0.9269\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5444 - acc: 0.9194 - val_loss: 1.5353 - val_acc: 0.9292\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5420 - acc: 0.9217 - val_loss: 1.5331 - val_acc: 0.9305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d65413dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W02SFm4xduWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "b6384529-e423-41b2-e532-f8ffb5d4d0bd"
      },
      "source": [
        "target_model600.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model600.save('weights_mnist_batch_100_15.h5')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5324 - acc: 0.9298 - val_loss: 1.5274 - val_acc: 0.9354\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5340 - acc: 0.9278 - val_loss: 1.5271 - val_acc: 0.9347\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5332 - acc: 0.9288 - val_loss: 1.5254 - val_acc: 0.9366\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5322 - acc: 0.9304 - val_loss: 1.5268 - val_acc: 0.9352\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5308 - acc: 0.9313 - val_loss: 1.5254 - val_acc: 0.9359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H4Qx6CaciY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn600():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model600 = target_model_fn600()\n",
        "target_model600.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJF6TxYXdybK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "d5323165-2c68-4f98-d946-43b4f63c775c"
      },
      "source": [
        "target_model1200.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=50)\n",
        "target_model1200.save('weights_mnist_batch_50_15.h5')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6058 - acc: 0.8550 - val_loss: 1.5972 - val_acc: 0.8634\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6099 - acc: 0.8508 - val_loss: 1.6079 - val_acc: 0.8530\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6143 - acc: 0.8462 - val_loss: 1.6151 - val_acc: 0.8448\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 77s 1ms/sample - loss: 1.6155 - acc: 0.8451 - val_loss: 1.6309 - val_acc: 0.8301\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6259 - acc: 0.8345 - val_loss: 1.6232 - val_acc: 0.8367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6fdR9EdVPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "94ee801b-5b19-4939-e00e-a9bde02ec4da"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 50\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 50\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn1200():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model1200 = target_model_fn1200()\n",
        "target_model1200.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 1.8524 - acc: 0.6826 - val_loss: 1.6399 - val_acc: 0.8400\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6193 - acc: 0.8521 - val_loss: 1.6040 - val_acc: 0.8644\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.5974 - acc: 0.8680 - val_loss: 1.5818 - val_acc: 0.8805\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 1.5902 - acc: 0.8726 - val_loss: 1.5829 - val_acc: 0.8801\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.5945 - acc: 0.8669 - val_loss: 1.5957 - val_acc: 0.8655\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 77s 1ms/sample - loss: 1.6012 - acc: 0.8608 - val_loss: 1.5993 - val_acc: 0.8616\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6006 - acc: 0.8611 - val_loss: 1.5848 - val_acc: 0.8761\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6028 - acc: 0.8580 - val_loss: 1.6025 - val_acc: 0.8592\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6081 - acc: 0.8528 - val_loss: 1.5956 - val_acc: 0.8647\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 76s 1ms/sample - loss: 1.6078 - acc: 0.8526 - val_loss: 1.5948 - val_acc: 0.8655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d652fae80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7dX9DLvYQRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "5be8d2bb-0765-4dc9-8dc6-f206404de974"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 2\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn2_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model2_clip = target_model_fn2_clip()\n",
        "target_model2_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.8960 - acc: 0.6380 - val_loss: 1.5990 - val_acc: 0.8893\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5817 - acc: 0.8936 - val_loss: 1.5521 - val_acc: 0.9157\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5549 - acc: 0.9131 - val_loss: 1.5418 - val_acc: 0.9241\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5461 - acc: 0.9187 - val_loss: 1.5406 - val_acc: 0.9236\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5418 - acc: 0.9216 - val_loss: 1.5334 - val_acc: 0.9296\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5412 - acc: 0.9221 - val_loss: 1.5306 - val_acc: 0.9344\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5404 - acc: 0.9222 - val_loss: 1.5376 - val_acc: 0.9241\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5398 - acc: 0.9231 - val_loss: 1.5335 - val_acc: 0.9284\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5397 - acc: 0.9222 - val_loss: 1.5380 - val_acc: 0.9223\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5446 - acc: 0.9173 - val_loss: 1.5394 - val_acc: 0.9230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d65206cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-yOjnjlg0GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "ab3d0c0b-807a-483a-8b9a-56996ecdb63b"
      },
      "source": [
        "target_model2_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model2_clip.save('weights_mnist_clip_2_15.h5')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5445 - acc: 0.9169 - val_loss: 1.5329 - val_acc: 0.9283\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5467 - acc: 0.9147 - val_loss: 1.5416 - val_acc: 0.9200\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5488 - acc: 0.9122 - val_loss: 1.5424 - val_acc: 0.9199\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5540 - acc: 0.9069 - val_loss: 1.5554 - val_acc: 0.9060\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5579 - acc: 0.9031 - val_loss: 1.5508 - val_acc: 0.9095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h__NF1acUPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "22e983d0-a28c-4e1e-b53c-5e64e853fed5"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 3\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn3_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model3_clip = target_model_fn3_clip()\n",
        "target_model3_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.9004 - acc: 0.5951 - val_loss: 1.6827 - val_acc: 0.7903\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6613 - acc: 0.8079 - val_loss: 1.6488 - val_acc: 0.8168\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6457 - acc: 0.8185 - val_loss: 1.6360 - val_acc: 0.8263\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6432 - acc: 0.8185 - val_loss: 1.6330 - val_acc: 0.8279\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6453 - acc: 0.8162 - val_loss: 1.6452 - val_acc: 0.8157\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6465 - acc: 0.8147 - val_loss: 1.6397 - val_acc: 0.8204\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6521 - acc: 0.8085 - val_loss: 1.6543 - val_acc: 0.8059\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6556 - acc: 0.8051 - val_loss: 1.6426 - val_acc: 0.8192\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6506 - acc: 0.8100 - val_loss: 1.6410 - val_acc: 0.8189\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6541 - acc: 0.8063 - val_loss: 1.6496 - val_acc: 0.8101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d64fafcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqo-m-54g51J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model3_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model3_clip.save('weights_mnist_clip_3_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W24Y59qOcZ00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "fb9650ac-ab5a-45b3-be97-2184ac4faaec"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 4\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn4_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model4_clip = target_model_fn4_clip()\n",
        "target_model4_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 1.9682 - acc: 0.5062 - val_loss: 1.7350 - val_acc: 0.7349\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7287 - acc: 0.7351 - val_loss: 1.7142 - val_acc: 0.7479\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7294 - acc: 0.7318 - val_loss: 1.7264 - val_acc: 0.7334\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7362 - acc: 0.7242 - val_loss: 1.7400 - val_acc: 0.7206\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7429 - acc: 0.7169 - val_loss: 1.7456 - val_acc: 0.7137\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7444 - acc: 0.7152 - val_loss: 1.7338 - val_acc: 0.7265\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7468 - acc: 0.7132 - val_loss: 1.7390 - val_acc: 0.7206\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7541 - acc: 0.7060 - val_loss: 1.7422 - val_acc: 0.7181\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7649 - acc: 0.6952 - val_loss: 1.7859 - val_acc: 0.6745\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7803 - acc: 0.6799 - val_loss: 1.7802 - val_acc: 0.6797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d64e54278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj6LKDpvg9Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model4_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model4_clip.save('weights_mnist_clip_4_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-TS8b2acc73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "20fadedb-4386-4dc2-e1f1-598e4fb2c623"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 5\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn5_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model5_clip = target_model_fn5_clip()\n",
        "target_model5_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.8886 - acc: 0.5908 - val_loss: 1.7588 - val_acc: 0.7057\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.7745 - acc: 0.6867 - val_loss: 1.7744 - val_acc: 0.6875\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.7889 - acc: 0.6711 - val_loss: 1.7844 - val_acc: 0.6753\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7978 - acc: 0.6618 - val_loss: 1.7849 - val_acc: 0.6744\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.7981 - acc: 0.6618 - val_loss: 1.7942 - val_acc: 0.6660\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.8010 - acc: 0.6591 - val_loss: 1.8045 - val_acc: 0.6556\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.8190 - acc: 0.6410 - val_loss: 1.8281 - val_acc: 0.6317\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.8040 - acc: 0.6561 - val_loss: 1.7721 - val_acc: 0.6892\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7795 - acc: 0.6808 - val_loss: 1.7801 - val_acc: 0.6807\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.8039 - acc: 0.6563 - val_loss: 1.8098 - val_acc: 0.6507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d64d254a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1zw7MqHg__U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model5_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model5_clip.save('weights_mnist_clip_5_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS8yYPaRx72f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "eb5cff1a-077b-409d-afe7-9fd83103f9fe"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 0.5\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn05_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model05_clip = target_model_fn05_clip()\n",
        "target_model05_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)\n",
        "target_model05_clip.save('weights_mnist_10_005_01_100_01.h5')\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 2.1303 - acc: 0.4502 - val_loss: 1.8913 - val_acc: 0.6560\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.7724 - acc: 0.7600 - val_loss: 1.6861 - val_acc: 0.8213\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.6556 - acc: 0.8401 - val_loss: 1.6202 - val_acc: 0.8641\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6103 - acc: 0.8699 - val_loss: 1.5916 - val_acc: 0.8851\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5882 - acc: 0.8854 - val_loss: 1.5785 - val_acc: 0.8921\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5771 - acc: 0.8934 - val_loss: 1.5689 - val_acc: 0.9000\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5689 - acc: 0.8998 - val_loss: 1.5620 - val_acc: 0.9044\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5629 - acc: 0.9044 - val_loss: 1.5578 - val_acc: 0.9075\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5575 - acc: 0.9091 - val_loss: 1.5515 - val_acc: 0.9135\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5531 - acc: 0.9121 - val_loss: 1.5490 - val_acc: 0.9159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seTFO27UhEDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "00de5209-c5b7-4d53-c823-07f972b8b952"
      },
      "source": [
        "target_model05_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model05_clip.save('weights_mnist_clip_05_15.h5')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5506 - acc: 0.9149 - val_loss: 1.5473 - val_acc: 0.9152\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.5478 - acc: 0.9167 - val_loss: 1.5436 - val_acc: 0.9205\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5450 - acc: 0.9197 - val_loss: 1.5418 - val_acc: 0.9225\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5417 - acc: 0.9223 - val_loss: 1.5378 - val_acc: 0.9260\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5388 - acc: 0.9248 - val_loss: 1.5342 - val_acc: 0.9297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZfiJtnk_gI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "300bbbd9-dfdc-4005-ad7b-16141ca61427"
      },
      "source": [
        "target_model15_clip.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model15_clip.save('weights_mnist_clip_15_15.h5')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5434 - acc: 0.9190 - val_loss: 1.5342 - val_acc: 0.9295\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5441 - acc: 0.9179 - val_loss: 1.5356 - val_acc: 0.9262\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5407 - acc: 0.9215 - val_loss: 1.5323 - val_acc: 0.9306\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5389 - acc: 0.9228 - val_loss: 1.5360 - val_acc: 0.9261\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5390 - acc: 0.9228 - val_loss: 1.5350 - val_acc: 0.9263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0a-L8bMcxO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "3b84c810-1ea3-4649-88c1-1fd1b6f6d35c"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn005_lr():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model005_lr = target_model_fn005_lr()\n",
        "target_model005_lr.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 2.2016 - acc: 0.3956 - val_loss: 1.9763 - val_acc: 0.6370\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.8436 - acc: 0.7175 - val_loss: 1.7363 - val_acc: 0.7845\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.6966 - acc: 0.8134 - val_loss: 1.6499 - val_acc: 0.8466\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.6364 - acc: 0.8506 - val_loss: 1.6107 - val_acc: 0.8675\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.6062 - acc: 0.8712 - val_loss: 1.5869 - val_acc: 0.8876\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5908 - acc: 0.8813 - val_loss: 1.5772 - val_acc: 0.8940\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5813 - acc: 0.8895 - val_loss: 1.5681 - val_acc: 0.9021\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5740 - acc: 0.8943 - val_loss: 1.5614 - val_acc: 0.9057\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5685 - acc: 0.8985 - val_loss: 1.5575 - val_acc: 0.9082\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5634 - acc: 0.9025 - val_loss: 1.5541 - val_acc: 0.9105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d6498ef60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mM8vBqoc3cv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "03fc2a42-c47b-4779-d135-dfab35aa7d58"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.15\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn015_lr():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model015_lr = target_model_fn015_lr()\n",
        "target_model015_lr.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.9590 - acc: 0.6019 - val_loss: 1.6866 - val_acc: 0.8134\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6354 - acc: 0.8468 - val_loss: 1.5989 - val_acc: 0.8751\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5880 - acc: 0.8812 - val_loss: 1.5708 - val_acc: 0.8963\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5698 - acc: 0.8965 - val_loss: 1.5538 - val_acc: 0.9115\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5577 - acc: 0.9071 - val_loss: 1.5477 - val_acc: 0.9163\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 77s 1ms/sample - loss: 1.5515 - acc: 0.9129 - val_loss: 1.5392 - val_acc: 0.9250\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5478 - acc: 0.9158 - val_loss: 1.5364 - val_acc: 0.9266\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5456 - acc: 0.9174 - val_loss: 1.5381 - val_acc: 0.9239\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5441 - acc: 0.9188 - val_loss: 1.5357 - val_acc: 0.9286\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5451 - acc: 0.9172 - val_loss: 1.5370 - val_acc: 0.9263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d646a3a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKzpGGRqyG1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "f1931c3d-9682-47ef-c9ab-3b9c28af9707"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 0.5\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn05_n():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model05_n = target_model_fn05_n()\n",
        "target_model05_n.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 1.9687 - acc: 0.6263 - val_loss: 1.6829 - val_acc: 0.8272\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.6268 - acc: 0.8648 - val_loss: 1.5831 - val_acc: 0.8927\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5795 - acc: 0.8932 - val_loss: 1.5627 - val_acc: 0.9039\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5628 - acc: 0.9054 - val_loss: 1.5496 - val_acc: 0.9165\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5537 - acc: 0.9123 - val_loss: 1.5435 - val_acc: 0.9222\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5480 - acc: 0.9174 - val_loss: 1.5380 - val_acc: 0.9268\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5440 - acc: 0.9204 - val_loss: 1.5353 - val_acc: 0.9295\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5397 - acc: 0.9245 - val_loss: 1.5304 - val_acc: 0.9348\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5364 - acc: 0.9273 - val_loss: 1.5279 - val_acc: 0.9366\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5337 - acc: 0.9295 - val_loss: 1.5252 - val_acc: 0.9386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d643810f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Iriptiez5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "e327544e-608b-41d7-8442-23bd2430fb28"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 2\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn2_n():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model2_n = target_model_fn2_n()\n",
        "target_model2_n.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 2.0173 - acc: 0.5480 - val_loss: 1.7140 - val_acc: 0.8120\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6581 - acc: 0.8338 - val_loss: 1.6104 - val_acc: 0.8648\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5976 - acc: 0.8737 - val_loss: 1.5731 - val_acc: 0.8978\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5807 - acc: 0.8881 - val_loss: 1.5710 - val_acc: 0.8976\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5734 - acc: 0.8928 - val_loss: 1.5644 - val_acc: 0.9003\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5723 - acc: 0.8918 - val_loss: 1.5641 - val_acc: 0.9000\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5693 - acc: 0.8941 - val_loss: 1.5578 - val_acc: 0.9047\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5660 - acc: 0.8963 - val_loss: 1.5596 - val_acc: 0.9033\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.5686 - acc: 0.8938 - val_loss: 1.5540 - val_acc: 0.9080\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5645 - acc: 0.8981 - val_loss: 1.5630 - val_acc: 0.8990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d63fc9390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG_FUoBfe3dI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "bdde645b-ad6e-424b-859f-03087ba160da"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1.5\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn15_n():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model15_n = target_model_fn15_n()\n",
        "target_model15_n.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 2.0241 - acc: 0.5630 - val_loss: 1.7252 - val_acc: 0.8099\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.6451 - acc: 0.8535 - val_loss: 1.5938 - val_acc: 0.8850\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5870 - acc: 0.8867 - val_loss: 1.5729 - val_acc: 0.8974\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5713 - acc: 0.8976 - val_loss: 1.5583 - val_acc: 0.9102\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5600 - acc: 0.9060 - val_loss: 1.5514 - val_acc: 0.9145\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5551 - acc: 0.9098 - val_loss: 1.5461 - val_acc: 0.9185\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5496 - acc: 0.9151 - val_loss: 1.5452 - val_acc: 0.9194\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5457 - acc: 0.9187 - val_loss: 1.5430 - val_acc: 0.9202\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5444 - acc: 0.9190 - val_loss: 1.5395 - val_acc: 0.9238\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5447 - acc: 0.9183 - val_loss: 1.5381 - val_acc: 0.9233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d63e773c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m890Nrb_qIV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "aea34322-9ba1-4313-aae7-2e4200dc2e14"
      },
      "source": [
        "target_model15_n.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model15_n.save('weights_mnist_noise_15_15.h5')\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5427 - acc: 0.9200 - val_loss: 1.5366 - val_acc: 0.9262\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5454 - acc: 0.9168 - val_loss: 1.5453 - val_acc: 0.9184\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5471 - acc: 0.9150 - val_loss: 1.5404 - val_acc: 0.9218\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5486 - acc: 0.9136 - val_loss: 1.5432 - val_acc: 0.9191\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5460 - acc: 0.9158 - val_loss: 1.5394 - val_acc: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQErmuxVqN05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "6979ff02-c7de-4dee-d8a7-b3b392735c23"
      },
      "source": [
        "target_model05_n.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model05_n.save('weights_mnist_noise_05_15.h5')\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5310 - acc: 0.9323 - val_loss: 1.5225 - val_acc: 0.9413\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5287 - acc: 0.9347 - val_loss: 1.5202 - val_acc: 0.9433\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5261 - acc: 0.9371 - val_loss: 1.5172 - val_acc: 0.9461\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5246 - acc: 0.9383 - val_loss: 1.5154 - val_acc: 0.9475\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5226 - acc: 0.9398 - val_loss: 1.5147 - val_acc: 0.9481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaV1aIzwqQy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "d2820845-9f12-4486-ac6d-dce1cbc02bf2"
      },
      "source": [
        "target_model2_n.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model2_n.save('weights_mnist_noise_2_15.h5')\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5659 - acc: 0.8957 - val_loss: 1.5565 - val_acc: 0.9055\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5737 - acc: 0.8880 - val_loss: 1.5760 - val_acc: 0.8858\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5808 - acc: 0.8801 - val_loss: 1.5713 - val_acc: 0.8884\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5802 - acc: 0.8811 - val_loss: 1.5775 - val_acc: 0.8847\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5891 - acc: 0.8716 - val_loss: 1.5848 - val_acc: 0.8756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDzG5q6MfBFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "4cf1d016-9326-45d3-dff0-523cf02b202b"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "def target_model_fn15_clip():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "target_model15_clip = target_model_fn15_clip()\n",
        "target_model15_clip.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 1.9476 - acc: 0.6159 - val_loss: 1.6419 - val_acc: 0.8579\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.6060 - acc: 0.8760 - val_loss: 1.5633 - val_acc: 0.9107\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.5644 - acc: 0.9058 - val_loss: 1.5444 - val_acc: 0.9257\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5501 - acc: 0.9160 - val_loss: 1.5411 - val_acc: 0.9244\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5477 - acc: 0.9177 - val_loss: 1.5350 - val_acc: 0.9291\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5424 - acc: 0.9219 - val_loss: 1.5340 - val_acc: 0.9293\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5423 - acc: 0.9205 - val_loss: 1.5347 - val_acc: 0.9285\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.5406 - acc: 0.9228 - val_loss: 1.5324 - val_acc: 0.9313\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.5395 - acc: 0.9227 - val_loss: 1.5330 - val_acc: 0.9285\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 74s 1ms/sample - loss: 1.5424 - acc: 0.9200 - val_loss: 1.5363 - val_acc: 0.9263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d63d02940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExsO1BKu-_lT",
        "colab_type": "text"
      },
      "source": [
        "Change Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WBzKTYY6FM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "eb595bfa-58ed-41fb-e1ec-1e9f161889ee"
      },
      "source": [
        "target_model005_lr.fit(X_train, y_train,\n",
        "          epochs=15,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 2.1233 - acc: 0.5190 - val_loss: 1.8746 - val_acc: 0.7080\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.7676 - acc: 0.7712 - val_loss: 1.6931 - val_acc: 0.8120\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6658 - acc: 0.8292 - val_loss: 1.6312 - val_acc: 0.8564\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6233 - acc: 0.8595 - val_loss: 1.6044 - val_acc: 0.8747\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.6011 - acc: 0.8757 - val_loss: 1.5853 - val_acc: 0.8866\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5866 - acc: 0.8853 - val_loss: 1.5719 - val_acc: 0.8987\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5771 - acc: 0.8922 - val_loss: 1.5657 - val_acc: 0.9031\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5713 - acc: 0.8963 - val_loss: 1.5596 - val_acc: 0.9061\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5664 - acc: 0.9007 - val_loss: 1.5551 - val_acc: 0.9128\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5627 - acc: 0.9034 - val_loss: 1.5530 - val_acc: 0.9117\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5592 - acc: 0.9061 - val_loss: 1.5490 - val_acc: 0.9170\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5564 - acc: 0.9080 - val_loss: 1.5471 - val_acc: 0.9193\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5535 - acc: 0.9112 - val_loss: 1.5439 - val_acc: 0.9203\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5516 - acc: 0.9128 - val_loss: 1.5426 - val_acc: 0.9219\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5494 - acc: 0.9146 - val_loss: 1.5408 - val_acc: 0.9234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d6405fe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXWC_1eon8HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model005_lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHd3gtqq6y2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "c011a0b6-5960-4b75-ebfa-5fcfd5de7167"
      },
      "source": [
        "target_model600.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5395 - acc: 0.9235 - val_loss: 1.5320 - val_acc: 0.9304\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5374 - acc: 0.9254 - val_loss: 1.5299 - val_acc: 0.9325\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5355 - acc: 0.9268 - val_loss: 1.5280 - val_acc: 0.9350\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5351 - acc: 0.9271 - val_loss: 1.5253 - val_acc: 0.9372\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5333 - acc: 0.9292 - val_loss: 1.5253 - val_acc: 0.9372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d6405fd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hodMKQg--OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "07bd155d-3fc0-4644-cc1d-103f8912c554"
      },
      "source": [
        "target_model015_lr.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=100)\n",
        "target_model015_lr.save('weights_mnist_lr_15_15.h5')\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5455 - acc: 0.9158 - val_loss: 1.5386 - val_acc: 0.9223\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5470 - acc: 0.9143 - val_loss: 1.5452 - val_acc: 0.9177\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5479 - acc: 0.9134 - val_loss: 1.5457 - val_acc: 0.9162\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5477 - acc: 0.9134 - val_loss: 1.5413 - val_acc: 0.9204\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5451 - acc: 0.9162 - val_loss: 1.5424 - val_acc: 0.9196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOBkLjWfogr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model005_lr.save('weights_mnist_lr_05_15.h5')"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0BGxY_QIeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1\n",
        "num_microbatches = 100\n",
        "learning_rate = 0.15\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1wq3anlQv8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVUq67_jQjFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_model1200.save('weights_mnist_10_01_01_50_01.h5')"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPWiqXGkH0aX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b95ff1f-5b65-42d4-91c2-9b9fb8e25952"
      },
      "source": [
        "# Test the success of the attack.\n",
        "\n",
        "# Prepare examples that were in the training, and out of the training.\n",
        "data_in = X_train[:4000], y_train[:4000]\n",
        "data_out = X_test[:4000], y_test[:4000]\n",
        "\n",
        "# Compile them into the expected format for the AttackModelBundle.\n",
        "attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "    target_model, data_in, data_out\n",
        ")\n",
        "\n",
        "# Compute the attack accuracy.\n",
        "attack_guesses = amb.predict(attack_test_data)\n",
        "attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "print(attack_accuracy)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.513375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOyEyCXPM-Hs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "04072baa-ec86-4f8b-e67b-581fd8919eb0"
      },
      "source": [
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=2000, noise_multiplier=3, epochs=10, delta=1e-5)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DP-SGD with sampling rate = 3.33% and noise_multiplier = 3 iterated over 300 steps satisfies differential privacy with eps = 1.02 and delta = 1e-05.\n",
            "The optimal RDP order is 23.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0156246807062732, 23.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    }
  ]
}