{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dp-mnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrTHSrxmHpN0K01Tz6AhWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbht98/attack_cat_or_dog_classification/blob/master/dp_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G5JzSrR-UWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "d76691ec-c040-466a-a268-15b9b1927800"
      },
      "source": [
        "!git clone https://github.com/tensorflow/privacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'privacy'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)\u001b[K\rremote: Counting objects:  11% (2/18)\u001b[K\rremote: Counting objects:  16% (3/18)\u001b[K\rremote: Counting objects:  22% (4/18)\u001b[K\rremote: Counting objects:  27% (5/18)\u001b[K\rremote: Counting objects:  33% (6/18)\u001b[K\rremote: Counting objects:  38% (7/18)\u001b[K\rremote: Counting objects:  44% (8/18)\u001b[K\rremote: Counting objects:  50% (9/18)\u001b[K\rremote: Counting objects:  55% (10/18)\u001b[K\rremote: Counting objects:  61% (11/18)\u001b[K\rremote: Counting objects:  66% (12/18)\u001b[K\rremote: Counting objects:  72% (13/18)\u001b[K\rremote: Counting objects:  77% (14/18)\u001b[K\rremote: Counting objects:  83% (15/18)\u001b[K\rremote: Counting objects:  88% (16/18)\u001b[K\rremote: Counting objects:  94% (17/18)\u001b[K\rremote: Counting objects: 100% (18/18)\u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 1773 (delta 5), reused 9 (delta 3), pack-reused 1755\u001b[K\n",
            "Receiving objects: 100% (1773/1773), 593.80 KiB | 852.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1219/1219), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdiNCDL7-Wee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "93946a9b-7ea7-4120-a34d-39899becb709"
      },
      "source": [
        "%cd privacy\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/privacy\n",
            "Obtaining file:///content/privacy\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy==0.3.0) (1.1.0)\n",
            "Collecting dm-tree~=0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/48/10fb721334810081b7e6eebeba0d12e12126c76993e8c243062d2f56a89f/dm_tree-0.1.5-cp36-cp36m-manylinux1_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow-privacy==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from dm-tree~=0.1.1->tensorflow-privacy==0.3.0) (1.12.0)\n",
            "Installing collected packages: dm-tree, tensorflow-privacy\n",
            "  Found existing installation: tensorflow-privacy 0.2.2\n",
            "    Uninstalling tensorflow-privacy-0.2.2:\n",
            "      Successfully uninstalled tensorflow-privacy-0.2.2\n",
            "  Running setup.py develop for tensorflow-privacy\n",
            "Successfully installed dm-tree-0.1.5 tensorflow-privacy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mliGCQoa-uR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d228efaa-29d4-451c-dd4c-8405ed10d5d3"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLTYxT6c-y4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "0fa4a67d-20f0-4830-a9dd-8f348798e784"
      },
      "source": [
        "!pip install tensorflow_privacy\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_privacy in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow_privacy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_2mi8J--1BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
        "    train, test = tf.keras.datasets.mnist.load_data()\n",
        "    train_data, train_labels = train\n",
        "    test_data, test_labels = test\n",
        "\n",
        "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "    train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "    test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "    train_labels = np.array(train_labels, dtype=np.int32)\n",
        "    test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "    assert train_data.min() == 0.\n",
        "    assert train_data.max() == 1.\n",
        "    assert test_data.min() == 0.\n",
        "    assert test_data.max() == 1.\n",
        "    return (train_data, train_labels), (test_data, test_labels)\n",
        "# Get MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = get_data()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRyb6swl-6t8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 15\n",
        "batch_size = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwFp_chf--Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = 1.3\n",
        "num_microbatches = 250\n",
        "learning_rate = 0.25\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pbbd0Th_AWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_model_fn():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUBVtcLB_DTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "26237fd4-10d1-4c63-f198-56487cd65085"
      },
      "source": [
        "target_model = target_model_fn()\n",
        "target_model.fit(X_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size)\n",
        "# target_model.load_weights(\"weights.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.9476 - acc: 0.5913 - val_loss: 1.6578 - val_acc: 0.8323\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.6114 - acc: 0.8698 - val_loss: 1.5675 - val_acc: 0.9031\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5677 - acc: 0.9005 - val_loss: 1.5513 - val_acc: 0.9164\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5518 - acc: 0.9150 - val_loss: 1.5370 - val_acc: 0.9274\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5413 - acc: 0.9236 - val_loss: 1.5303 - val_acc: 0.9354\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5355 - acc: 0.9288 - val_loss: 1.5290 - val_acc: 0.9356\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5328 - acc: 0.9312 - val_loss: 1.5278 - val_acc: 0.9367\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5306 - acc: 0.9327 - val_loss: 1.5258 - val_acc: 0.9370\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5296 - acc: 0.9332 - val_loss: 1.5250 - val_acc: 0.9371\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5288 - acc: 0.9344 - val_loss: 1.5236 - val_acc: 0.9378\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5272 - acc: 0.9357 - val_loss: 1.5218 - val_acc: 0.9396\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5274 - acc: 0.9350 - val_loss: 1.5215 - val_acc: 0.9398\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5258 - acc: 0.9366 - val_loss: 1.5206 - val_acc: 0.9416\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.5231 - acc: 0.9392 - val_loss: 1.5180 - val_acc: 0.9438\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 1.5229 - acc: 0.9391 - val_loss: 1.5139 - val_acc: 0.9481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09ed4b4eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGtkI7X1D0k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "target_model.save('weights1.h5')\n",
        "files.download('weights1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Wz1SpRWGd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "f5fe7b07-5f3e-4a22-d5b7-101b5198cb3b"
      },
      "source": [
        "!pip3 install mia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mia\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/12/f149a7cd43e49725921e9884363aa3cbfea8a49c319a944eb71d48973fa9/mia-0.1.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mia) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from mia) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mia) (0.22.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from mia) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mia) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mia) (0.15.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->mia) (0.16.0)\n",
            "Building wheels for collected packages: mia\n",
            "  Building wheel for mia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mia: filename=mia-0.1.2-cp36-none-any.whl size=11079 sha256=352756313b8267d06001c6ec4e59bcf642cf5cd66afe25319bba4f9ae613e4ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/83/e4/baae7782aa0d2e45af485d25a7994bab3f76428e483252ce82\n",
            "Successfully built mia\n",
            "Installing collected packages: mia\n",
            "Successfully installed mia-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_mNxh2XW-mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library \n",
        "import numpy as np\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBq0Rx_eXHbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 10\n",
        "SHADOW_DATASET_SIZE = 4000\n",
        "ATTACK_TEST_DATASET_SIZE = 4000\n",
        "\n",
        "\n",
        "TARGET_EPOCHS = 12 # Number of epochs to train target and shadow models\n",
        "ATTACK_EPOCHS = 12 # Number of epochs to train attack models.\n",
        "NUM_SHADOWS = 3 # Number of epochs to train attack models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7ftSr-iWQiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attack_model_fn():\n",
        "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
        "    Following the original paper, this attack model is specific to the class of the input.\n",
        "    AttachModelBundle creates multiple instances of this model for each class.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
        "\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk8QXTPiWhW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ff9db84-9ebc-4001-a2ae-3fd88560b52d"
      },
      "source": [
        "# Train the shadow models.\n",
        "smb = ShadowModelBundle(\n",
        "    target_model_fn,\n",
        "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "    num_models=NUM_SHADOWS,\n",
        ")\n",
        "\n",
        "# We assume that attacker's data were not seen in target's training.\n",
        "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
        "    X_test, y_test, test_size=0.1\n",
        ")\n",
        "print(attacker_X_train.shape, attacker_X_test.shape)\n",
        "\n",
        "print(\"Training the shadow models...\")\n",
        "X_shadow, y_shadow = smb.fit_transform(\n",
        "    attacker_X_train,\n",
        "    attacker_y_train,\n",
        "    fit_kwargs=dict(\n",
        "        epochs=TARGET_EPOCHS,\n",
        "        verbose=True,\n",
        "        validation_data=(attacker_X_test, attacker_y_test),\n",
        "        batch_size=250\n",
        "    ),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9000, 28, 28, 1) (1000, 28, 28, 1)\n",
            "Training the shadow models...\n",
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.3015 - acc: 0.1107 - val_loss: 2.2988 - val_acc: 0.1010\n",
            "Epoch 2/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2931 - acc: 0.1382 - val_loss: 2.2865 - val_acc: 0.1930\n",
            "Epoch 3/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2746 - acc: 0.2368 - val_loss: 2.2607 - val_acc: 0.2740\n",
            "Epoch 4/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2348 - acc: 0.4072 - val_loss: 2.1945 - val_acc: 0.5960\n",
            "Epoch 5/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.1423 - acc: 0.6118 - val_loss: 2.0721 - val_acc: 0.6740\n",
            "Epoch 6/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.0097 - acc: 0.6618 - val_loss: 1.9334 - val_acc: 0.6570\n",
            "Epoch 7/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.9015 - acc: 0.6837 - val_loss: 1.8407 - val_acc: 0.7350\n",
            "Epoch 8/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.8308 - acc: 0.7295 - val_loss: 1.7851 - val_acc: 0.7580\n",
            "Epoch 9/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7884 - acc: 0.7508 - val_loss: 1.7478 - val_acc: 0.7700\n",
            "Epoch 10/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7482 - acc: 0.7853 - val_loss: 1.7246 - val_acc: 0.8080\n",
            "Epoch 11/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7192 - acc: 0.8062 - val_loss: 1.6950 - val_acc: 0.8200\n",
            "Epoch 12/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7013 - acc: 0.8152 - val_loss: 1.6787 - val_acc: 0.8340\n",
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.3001 - acc: 0.0925 - val_loss: 2.2944 - val_acc: 0.1050\n",
            "Epoch 2/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2908 - acc: 0.0925 - val_loss: 2.2817 - val_acc: 0.1060\n",
            "Epoch 3/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2692 - acc: 0.1875 - val_loss: 2.2419 - val_acc: 0.3610\n",
            "Epoch 4/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2110 - acc: 0.4205 - val_loss: 2.1508 - val_acc: 0.5560\n",
            "Epoch 5/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.1002 - acc: 0.5435 - val_loss: 2.0092 - val_acc: 0.5780\n",
            "Epoch 6/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.9737 - acc: 0.6302 - val_loss: 1.9028 - val_acc: 0.6970\n",
            "Epoch 7/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.8862 - acc: 0.6845 - val_loss: 1.8319 - val_acc: 0.7340\n",
            "Epoch 8/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.8279 - acc: 0.7287 - val_loss: 1.7992 - val_acc: 0.7200\n",
            "Epoch 9/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7917 - acc: 0.7483 - val_loss: 1.7527 - val_acc: 0.7890\n",
            "Epoch 10/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7588 - acc: 0.7685 - val_loss: 1.7319 - val_acc: 0.7860\n",
            "Epoch 11/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7318 - acc: 0.7790 - val_loss: 1.7144 - val_acc: 0.7920\n",
            "Epoch 12/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7149 - acc: 0.7845 - val_loss: 1.6882 - val_acc: 0.8140\n",
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.3004 - acc: 0.1222 - val_loss: 2.2993 - val_acc: 0.0920\n",
            "Epoch 2/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2963 - acc: 0.1150 - val_loss: 2.2947 - val_acc: 0.1040\n",
            "Epoch 3/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2890 - acc: 0.1443 - val_loss: 2.2835 - val_acc: 0.1440\n",
            "Epoch 4/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2718 - acc: 0.2595 - val_loss: 2.2586 - val_acc: 0.3840\n",
            "Epoch 5/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.2333 - acc: 0.4473 - val_loss: 2.1877 - val_acc: 0.4710\n",
            "Epoch 6/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.1467 - acc: 0.5383 - val_loss: 2.0961 - val_acc: 0.5770\n",
            "Epoch 7/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 2.0353 - acc: 0.6180 - val_loss: 1.9796 - val_acc: 0.6880\n",
            "Epoch 8/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.9578 - acc: 0.6365 - val_loss: 1.8832 - val_acc: 0.7300\n",
            "Epoch 9/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.8830 - acc: 0.6820 - val_loss: 1.8402 - val_acc: 0.7340\n",
            "Epoch 10/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.8439 - acc: 0.6860 - val_loss: 1.7881 - val_acc: 0.7690\n",
            "Epoch 11/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7940 - acc: 0.7415 - val_loss: 1.7726 - val_acc: 0.7660\n",
            "Epoch 12/12\n",
            "4000/4000 [==============================] - 5s 1ms/sample - loss: 1.7588 - acc: 0.7703 - val_loss: 1.7388 - val_acc: 0.8030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcqK1HVWkPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8140120f-db7f-4e6d-af9c-c98da97b500e"
      },
      "source": [
        "  amb = AttackModelBundle(attack_model_fn, num_classes=10)\n",
        "\n",
        "  # Fit the attack models.\n",
        "  print(\"Training the attack models...\")\n",
        "  amb.fit(\n",
        "      X_shadow, y_shadow, fit_kwargs=dict(epochs=15, verbose=True)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the attack models...\n",
            "Train on 2342 samples\n",
            "Epoch 1/15\n",
            "2342/2342 [==============================] - 0s 182us/sample - loss: 0.6942 - acc: 0.5077\n",
            "Epoch 2/15\n",
            "2342/2342 [==============================] - 0s 86us/sample - loss: 0.6943 - acc: 0.4923\n",
            "Epoch 3/15\n",
            "2342/2342 [==============================] - 0s 88us/sample - loss: 0.6946 - acc: 0.5013\n",
            "Epoch 4/15\n",
            "2342/2342 [==============================] - 0s 86us/sample - loss: 0.6938 - acc: 0.5013\n",
            "Epoch 5/15\n",
            "2342/2342 [==============================] - 0s 93us/sample - loss: 0.6932 - acc: 0.5107\n",
            "Epoch 6/15\n",
            "2342/2342 [==============================] - 0s 95us/sample - loss: 0.6934 - acc: 0.5021\n",
            "Epoch 7/15\n",
            "2342/2342 [==============================] - 0s 90us/sample - loss: 0.6936 - acc: 0.4991\n",
            "Epoch 8/15\n",
            "2342/2342 [==============================] - 0s 87us/sample - loss: 0.6929 - acc: 0.5056\n",
            "Epoch 9/15\n",
            "2342/2342 [==============================] - 0s 91us/sample - loss: 0.6932 - acc: 0.5111\n",
            "Epoch 10/15\n",
            "2342/2342 [==============================] - 0s 86us/sample - loss: 0.6932 - acc: 0.4910\n",
            "Epoch 11/15\n",
            "2342/2342 [==============================] - 0s 90us/sample - loss: 0.6927 - acc: 0.4910\n",
            "Epoch 12/15\n",
            "2342/2342 [==============================] - 0s 86us/sample - loss: 0.6924 - acc: 0.5064\n",
            "Epoch 13/15\n",
            "2342/2342 [==============================] - 0s 85us/sample - loss: 0.6923 - acc: 0.5073\n",
            "Epoch 14/15\n",
            "2342/2342 [==============================] - 0s 93us/sample - loss: 0.6927 - acc: 0.5064\n",
            "Epoch 15/15\n",
            "2342/2342 [==============================] - 0s 90us/sample - loss: 0.6933 - acc: 0.4983\n",
            "Train on 2706 samples\n",
            "Epoch 1/15\n",
            "2706/2706 [==============================] - 0s 167us/sample - loss: 0.6948 - acc: 0.4834\n",
            "Epoch 2/15\n",
            "2706/2706 [==============================] - 0s 92us/sample - loss: 0.6940 - acc: 0.4900\n",
            "Epoch 3/15\n",
            "2706/2706 [==============================] - 0s 87us/sample - loss: 0.6934 - acc: 0.5026\n",
            "Epoch 4/15\n",
            "2706/2706 [==============================] - 0s 87us/sample - loss: 0.6934 - acc: 0.4885\n",
            "Epoch 5/15\n",
            "2706/2706 [==============================] - 0s 85us/sample - loss: 0.6937 - acc: 0.4878\n",
            "Epoch 6/15\n",
            "2706/2706 [==============================] - 0s 90us/sample - loss: 0.6938 - acc: 0.4830\n",
            "Epoch 7/15\n",
            "2706/2706 [==============================] - 0s 91us/sample - loss: 0.6930 - acc: 0.5000\n",
            "Epoch 8/15\n",
            "2706/2706 [==============================] - 0s 90us/sample - loss: 0.6923 - acc: 0.4996\n",
            "Epoch 9/15\n",
            "2706/2706 [==============================] - 0s 87us/sample - loss: 0.6932 - acc: 0.4956\n",
            "Epoch 10/15\n",
            "2706/2706 [==============================] - 0s 88us/sample - loss: 0.6919 - acc: 0.5007\n",
            "Epoch 11/15\n",
            "2706/2706 [==============================] - 0s 91us/sample - loss: 0.6919 - acc: 0.4959\n",
            "Epoch 12/15\n",
            "2706/2706 [==============================] - 0s 85us/sample - loss: 0.6913 - acc: 0.5092\n",
            "Epoch 13/15\n",
            "2706/2706 [==============================] - 0s 88us/sample - loss: 0.6910 - acc: 0.5218\n",
            "Epoch 14/15\n",
            "2706/2706 [==============================] - 0s 88us/sample - loss: 0.6919 - acc: 0.5111\n",
            "Epoch 15/15\n",
            "2706/2706 [==============================] - 0s 92us/sample - loss: 0.6906 - acc: 0.5026\n",
            "Train on 2452 samples\n",
            "Epoch 1/15\n",
            "2452/2452 [==============================] - 0s 192us/sample - loss: 0.6945 - acc: 0.5012\n",
            "Epoch 2/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6935 - acc: 0.5061\n",
            "Epoch 3/15\n",
            "2452/2452 [==============================] - 0s 89us/sample - loss: 0.6934 - acc: 0.5090\n",
            "Epoch 4/15\n",
            "2452/2452 [==============================] - 0s 85us/sample - loss: 0.6937 - acc: 0.4963\n",
            "Epoch 5/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6931 - acc: 0.5163\n",
            "Epoch 6/15\n",
            "2452/2452 [==============================] - 0s 90us/sample - loss: 0.6936 - acc: 0.4971\n",
            "Epoch 7/15\n",
            "2452/2452 [==============================] - 0s 93us/sample - loss: 0.6917 - acc: 0.5147\n",
            "Epoch 8/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6928 - acc: 0.5094\n",
            "Epoch 9/15\n",
            "2452/2452 [==============================] - 0s 92us/sample - loss: 0.6915 - acc: 0.5131\n",
            "Epoch 10/15\n",
            "2452/2452 [==============================] - 0s 90us/sample - loss: 0.6916 - acc: 0.5098\n",
            "Epoch 11/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6913 - acc: 0.5016\n",
            "Epoch 12/15\n",
            "2452/2452 [==============================] - 0s 88us/sample - loss: 0.6894 - acc: 0.5163\n",
            "Epoch 13/15\n",
            "2452/2452 [==============================] - 0s 86us/sample - loss: 0.6914 - acc: 0.5192\n",
            "Epoch 14/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6892 - acc: 0.5277\n",
            "Epoch 15/15\n",
            "2452/2452 [==============================] - 0s 91us/sample - loss: 0.6901 - acc: 0.5204\n",
            "Train on 2414 samples\n",
            "Epoch 1/15\n",
            "2414/2414 [==============================] - 0s 180us/sample - loss: 0.6948 - acc: 0.4764\n",
            "Epoch 2/15\n",
            "2414/2414 [==============================] - 0s 89us/sample - loss: 0.6936 - acc: 0.4959\n",
            "Epoch 3/15\n",
            "2414/2414 [==============================] - 0s 84us/sample - loss: 0.6930 - acc: 0.5033\n",
            "Epoch 4/15\n",
            "2414/2414 [==============================] - 0s 87us/sample - loss: 0.6934 - acc: 0.5066\n",
            "Epoch 5/15\n",
            "2414/2414 [==============================] - 0s 84us/sample - loss: 0.6926 - acc: 0.5108\n",
            "Epoch 6/15\n",
            "2414/2414 [==============================] - 0s 91us/sample - loss: 0.6921 - acc: 0.5211\n",
            "Epoch 7/15\n",
            "2414/2414 [==============================] - 0s 92us/sample - loss: 0.6925 - acc: 0.5116\n",
            "Epoch 8/15\n",
            "2414/2414 [==============================] - 0s 87us/sample - loss: 0.6913 - acc: 0.5178\n",
            "Epoch 9/15\n",
            "2414/2414 [==============================] - 0s 86us/sample - loss: 0.6921 - acc: 0.5062\n",
            "Epoch 10/15\n",
            "2414/2414 [==============================] - 0s 86us/sample - loss: 0.6923 - acc: 0.5120\n",
            "Epoch 11/15\n",
            "2414/2414 [==============================] - 0s 86us/sample - loss: 0.6915 - acc: 0.5145\n",
            "Epoch 12/15\n",
            "2414/2414 [==============================] - 0s 88us/sample - loss: 0.6917 - acc: 0.5178\n",
            "Epoch 13/15\n",
            "2414/2414 [==============================] - 0s 88us/sample - loss: 0.6916 - acc: 0.5050\n",
            "Epoch 14/15\n",
            "2414/2414 [==============================] - 0s 86us/sample - loss: 0.6906 - acc: 0.5224\n",
            "Epoch 15/15\n",
            "2414/2414 [==============================] - 0s 93us/sample - loss: 0.6903 - acc: 0.5211\n",
            "Train on 2400 samples\n",
            "Epoch 1/15\n",
            "2400/2400 [==============================] - 0s 180us/sample - loss: 0.6940 - acc: 0.5054\n",
            "Epoch 2/15\n",
            "2400/2400 [==============================] - 0s 87us/sample - loss: 0.6935 - acc: 0.5175\n",
            "Epoch 3/15\n",
            "2400/2400 [==============================] - 0s 89us/sample - loss: 0.6926 - acc: 0.5138\n",
            "Epoch 4/15\n",
            "2400/2400 [==============================] - 0s 90us/sample - loss: 0.6925 - acc: 0.5229\n",
            "Epoch 5/15\n",
            "2400/2400 [==============================] - 0s 87us/sample - loss: 0.6924 - acc: 0.5188\n",
            "Epoch 6/15\n",
            "2400/2400 [==============================] - 0s 84us/sample - loss: 0.6924 - acc: 0.5108\n",
            "Epoch 7/15\n",
            "2400/2400 [==============================] - 0s 86us/sample - loss: 0.6919 - acc: 0.5121\n",
            "Epoch 8/15\n",
            "2400/2400 [==============================] - 0s 86us/sample - loss: 0.6908 - acc: 0.5150\n",
            "Epoch 9/15\n",
            "2400/2400 [==============================] - 0s 92us/sample - loss: 0.6924 - acc: 0.5050\n",
            "Epoch 10/15\n",
            "2400/2400 [==============================] - 0s 84us/sample - loss: 0.6913 - acc: 0.5067\n",
            "Epoch 11/15\n",
            "2400/2400 [==============================] - 0s 86us/sample - loss: 0.6907 - acc: 0.5017\n",
            "Epoch 12/15\n",
            "2400/2400 [==============================] - 0s 89us/sample - loss: 0.6914 - acc: 0.5083\n",
            "Epoch 13/15\n",
            "2400/2400 [==============================] - 0s 87us/sample - loss: 0.6904 - acc: 0.5117\n",
            "Epoch 14/15\n",
            "2400/2400 [==============================] - 0s 89us/sample - loss: 0.6884 - acc: 0.5192\n",
            "Epoch 15/15\n",
            "2400/2400 [==============================] - 0s 87us/sample - loss: 0.6908 - acc: 0.5271\n",
            "Train on 2139 samples\n",
            "Epoch 1/15\n",
            "2139/2139 [==============================] - 0s 204us/sample - loss: 0.6940 - acc: 0.5054\n",
            "Epoch 2/15\n",
            "2139/2139 [==============================] - 0s 93us/sample - loss: 0.6935 - acc: 0.5086\n",
            "Epoch 3/15\n",
            "2139/2139 [==============================] - 0s 91us/sample - loss: 0.6936 - acc: 0.4979\n",
            "Epoch 4/15\n",
            "2139/2139 [==============================] - 0s 90us/sample - loss: 0.6933 - acc: 0.5077\n",
            "Epoch 5/15\n",
            "2139/2139 [==============================] - 0s 91us/sample - loss: 0.6932 - acc: 0.5054\n",
            "Epoch 6/15\n",
            "2139/2139 [==============================] - 0s 90us/sample - loss: 0.6930 - acc: 0.5166\n",
            "Epoch 7/15\n",
            "2139/2139 [==============================] - 0s 92us/sample - loss: 0.6936 - acc: 0.5035\n",
            "Epoch 8/15\n",
            "2139/2139 [==============================] - 0s 88us/sample - loss: 0.6926 - acc: 0.5049\n",
            "Epoch 9/15\n",
            "2139/2139 [==============================] - 0s 88us/sample - loss: 0.6931 - acc: 0.5189\n",
            "Epoch 10/15\n",
            "2139/2139 [==============================] - 0s 88us/sample - loss: 0.6922 - acc: 0.5283\n",
            "Epoch 11/15\n",
            "2139/2139 [==============================] - 0s 93us/sample - loss: 0.6920 - acc: 0.5143\n",
            "Epoch 12/15\n",
            "2139/2139 [==============================] - 0s 88us/sample - loss: 0.6908 - acc: 0.5278\n",
            "Epoch 13/15\n",
            "2139/2139 [==============================] - 0s 89us/sample - loss: 0.6931 - acc: 0.5016\n",
            "Epoch 14/15\n",
            "2139/2139 [==============================] - 0s 87us/sample - loss: 0.6916 - acc: 0.5227\n",
            "Epoch 15/15\n",
            "2139/2139 [==============================] - 0s 93us/sample - loss: 0.6905 - acc: 0.5213\n",
            "Train on 2267 samples\n",
            "Epoch 1/15\n",
            "2267/2267 [==============================] - 0s 191us/sample - loss: 0.6933 - acc: 0.5161\n",
            "Epoch 2/15\n",
            "2267/2267 [==============================] - 0s 90us/sample - loss: 0.6936 - acc: 0.5055\n",
            "Epoch 3/15\n",
            "2267/2267 [==============================] - 0s 86us/sample - loss: 0.6938 - acc: 0.4958\n",
            "Epoch 4/15\n",
            "2267/2267 [==============================] - 0s 91us/sample - loss: 0.6933 - acc: 0.5108\n",
            "Epoch 5/15\n",
            "2267/2267 [==============================] - 0s 89us/sample - loss: 0.6928 - acc: 0.5073\n",
            "Epoch 6/15\n",
            "2267/2267 [==============================] - 0s 87us/sample - loss: 0.6940 - acc: 0.4910\n",
            "Epoch 7/15\n",
            "2267/2267 [==============================] - 0s 91us/sample - loss: 0.6936 - acc: 0.4989\n",
            "Epoch 8/15\n",
            "2267/2267 [==============================] - 0s 86us/sample - loss: 0.6926 - acc: 0.5095\n",
            "Epoch 9/15\n",
            "2267/2267 [==============================] - 0s 94us/sample - loss: 0.6925 - acc: 0.5148\n",
            "Epoch 10/15\n",
            "2267/2267 [==============================] - 0s 86us/sample - loss: 0.6932 - acc: 0.4971\n",
            "Epoch 11/15\n",
            "2267/2267 [==============================] - 0s 90us/sample - loss: 0.6908 - acc: 0.5210\n",
            "Epoch 12/15\n",
            "2267/2267 [==============================] - 0s 88us/sample - loss: 0.6911 - acc: 0.5143\n",
            "Epoch 13/15\n",
            "2267/2267 [==============================] - 0s 91us/sample - loss: 0.6915 - acc: 0.5148\n",
            "Epoch 14/15\n",
            "2267/2267 [==============================] - 0s 94us/sample - loss: 0.6920 - acc: 0.5139\n",
            "Epoch 15/15\n",
            "2267/2267 [==============================] - 0s 86us/sample - loss: 0.6921 - acc: 0.4892\n",
            "Train on 2462 samples\n",
            "Epoch 1/15\n",
            "2462/2462 [==============================] - 0s 194us/sample - loss: 0.6945 - acc: 0.5016\n",
            "Epoch 2/15\n",
            "2462/2462 [==============================] - 0s 86us/sample - loss: 0.6937 - acc: 0.5041\n",
            "Epoch 3/15\n",
            "2462/2462 [==============================] - 0s 90us/sample - loss: 0.6929 - acc: 0.5146\n",
            "Epoch 4/15\n",
            "2462/2462 [==============================] - 0s 90us/sample - loss: 0.6932 - acc: 0.5203\n",
            "Epoch 5/15\n",
            "2462/2462 [==============================] - 0s 90us/sample - loss: 0.6923 - acc: 0.5053\n",
            "Epoch 6/15\n",
            "2462/2462 [==============================] - 0s 85us/sample - loss: 0.6936 - acc: 0.5069\n",
            "Epoch 7/15\n",
            "2462/2462 [==============================] - 0s 93us/sample - loss: 0.6924 - acc: 0.5199\n",
            "Epoch 8/15\n",
            "2462/2462 [==============================] - 0s 97us/sample - loss: 0.6925 - acc: 0.5252\n",
            "Epoch 9/15\n",
            "2462/2462 [==============================] - 0s 94us/sample - loss: 0.6916 - acc: 0.5150\n",
            "Epoch 10/15\n",
            "2462/2462 [==============================] - 0s 90us/sample - loss: 0.6915 - acc: 0.5232\n",
            "Epoch 11/15\n",
            "2462/2462 [==============================] - 0s 88us/sample - loss: 0.6924 - acc: 0.5114\n",
            "Epoch 12/15\n",
            "2462/2462 [==============================] - 0s 92us/sample - loss: 0.6905 - acc: 0.5292\n",
            "Epoch 13/15\n",
            "2462/2462 [==============================] - 0s 90us/sample - loss: 0.6920 - acc: 0.4972\n",
            "Epoch 14/15\n",
            "2462/2462 [==============================] - 0s 91us/sample - loss: 0.6912 - acc: 0.5024\n",
            "Epoch 15/15\n",
            "2462/2462 [==============================] - 0s 88us/sample - loss: 0.6912 - acc: 0.5240\n",
            "Train on 2354 samples\n",
            "Epoch 1/15\n",
            "2354/2354 [==============================] - 0s 201us/sample - loss: 0.6942 - acc: 0.4983\n",
            "Epoch 2/15\n",
            "2354/2354 [==============================] - 0s 91us/sample - loss: 0.6933 - acc: 0.4975\n",
            "Epoch 3/15\n",
            "2354/2354 [==============================] - 0s 92us/sample - loss: 0.6935 - acc: 0.4996\n",
            "Epoch 4/15\n",
            "2354/2354 [==============================] - 0s 92us/sample - loss: 0.6932 - acc: 0.5021\n",
            "Epoch 5/15\n",
            "2354/2354 [==============================] - 0s 95us/sample - loss: 0.6926 - acc: 0.5030\n",
            "Epoch 6/15\n",
            "2354/2354 [==============================] - 0s 90us/sample - loss: 0.6932 - acc: 0.5166\n",
            "Epoch 7/15\n",
            "2354/2354 [==============================] - 0s 93us/sample - loss: 0.6930 - acc: 0.5123\n",
            "Epoch 8/15\n",
            "2354/2354 [==============================] - 0s 88us/sample - loss: 0.6919 - acc: 0.5153\n",
            "Epoch 9/15\n",
            "2354/2354 [==============================] - 0s 95us/sample - loss: 0.6923 - acc: 0.5229\n",
            "Epoch 10/15\n",
            "2354/2354 [==============================] - 0s 96us/sample - loss: 0.6913 - acc: 0.5272\n",
            "Epoch 11/15\n",
            "2354/2354 [==============================] - 0s 89us/sample - loss: 0.6928 - acc: 0.5136\n",
            "Epoch 12/15\n",
            "2354/2354 [==============================] - 0s 93us/sample - loss: 0.6922 - acc: 0.5285\n",
            "Epoch 13/15\n",
            "2354/2354 [==============================] - 0s 90us/sample - loss: 0.6917 - acc: 0.5246\n",
            "Epoch 14/15\n",
            "2354/2354 [==============================] - 0s 93us/sample - loss: 0.6907 - acc: 0.5374\n",
            "Epoch 15/15\n",
            "2354/2354 [==============================] - 0s 91us/sample - loss: 0.6917 - acc: 0.5204\n",
            "Train on 2464 samples\n",
            "Epoch 1/15\n",
            "2464/2464 [==============================] - 1s 222us/sample - loss: 0.6946 - acc: 0.4919\n",
            "Epoch 2/15\n",
            "2464/2464 [==============================] - 0s 93us/sample - loss: 0.6933 - acc: 0.5012\n",
            "Epoch 3/15\n",
            "2464/2464 [==============================] - 0s 91us/sample - loss: 0.6937 - acc: 0.5065\n",
            "Epoch 4/15\n",
            "2464/2464 [==============================] - 0s 96us/sample - loss: 0.6932 - acc: 0.4959\n",
            "Epoch 5/15\n",
            "2464/2464 [==============================] - 0s 95us/sample - loss: 0.6925 - acc: 0.5134\n",
            "Epoch 6/15\n",
            "2464/2464 [==============================] - 0s 93us/sample - loss: 0.6917 - acc: 0.5049\n",
            "Epoch 7/15\n",
            "2464/2464 [==============================] - 0s 96us/sample - loss: 0.6908 - acc: 0.5288\n",
            "Epoch 8/15\n",
            "2464/2464 [==============================] - 0s 100us/sample - loss: 0.6912 - acc: 0.5223\n",
            "Epoch 9/15\n",
            "2464/2464 [==============================] - 0s 90us/sample - loss: 0.6909 - acc: 0.5244\n",
            "Epoch 10/15\n",
            "2464/2464 [==============================] - 0s 90us/sample - loss: 0.6899 - acc: 0.5191\n",
            "Epoch 11/15\n",
            "2464/2464 [==============================] - 0s 91us/sample - loss: 0.6901 - acc: 0.5227\n",
            "Epoch 12/15\n",
            "2464/2464 [==============================] - 0s 91us/sample - loss: 0.6895 - acc: 0.5260\n",
            "Epoch 13/15\n",
            "2464/2464 [==============================] - 0s 92us/sample - loss: 0.6901 - acc: 0.5191\n",
            "Epoch 14/15\n",
            "2464/2464 [==============================] - 0s 90us/sample - loss: 0.6891 - acc: 0.5207\n",
            "Epoch 15/15\n",
            "2464/2464 [==============================] - 0s 92us/sample - loss: 0.6886 - acc: 0.5191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ashUzO0Wl9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76673cd0-6d04-4712-af80-2a96832e0aa1"
      },
      "source": [
        "    # Test the success of the attack.\n",
        "\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = X_train[:4000], y_train[:4000]\n",
        "    data_out = X_test[:4000], y_test[:4000]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "        target_model, data_in, data_out\n",
        "    )\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = amb.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print(attack_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.506625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}